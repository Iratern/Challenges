{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "python-questions-ibrahim-askin",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibrahim-askin/Challenges/blob/main/python_questions_ibrahim_askin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<table align='right' style=\"margin-right:100px\" ><td>\n",
        "  <a target=\"_blank\"  href=\"https://www.weclouddata.com/\">\n",
        "    <img width=128px src=\"https://www.weclouddata.com/wp-content/uploads/2021/01/WCD-Logo.svg\" /></a>\n",
        "</td></table>\n",
        "\n",
        "## AI Bootcamp Entry Quiz"
      ],
      "metadata": {
        "id": "KbpF3B68jZxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hello! Contained within this notebook you'll find everything you need to pass the two-stage entry exam.\n",
        "\n",
        "What follows is 4 questions with an the intention of testing some of the criteria necesary to enjoy the upcoming bootcamp:\n",
        "\n",
        "\\\n",
        "\n",
        "- **Question 1 and 2** will test your ability to work with data structures and think through a problem in pure Python.\n",
        "\n",
        "- **Question 2** will have you interfacing with the underlying Numpy concepts that serve as a foundation for Pandas, thus pulling you into the mindset of data ingestion and prep ahead of the course.\n",
        "\n",
        "- **Question 3** will have you with regex and Pythonic data structures once again to have you thinking in the direction of Natural Language Processing ahead of your second module!\n",
        "\n",
        "\\\n",
        "\n",
        "Ultimately the above is here to test your Python knowldege and ability to employ libraries in context so don't worry at all if some of the overall context is unfamiliar to you, we're here to take you on that journey!  \n",
        "\n",
        "Last of all note that we'll be working in a Colaboratory environment just like this for the first two modules in order to leverage the GPU backend's available before moving towards frameworks for professional and industrial deployment. \n",
        "\n",
        "Once completed go ahead and:\n",
        "\n",
        "\\\n",
        "\n",
        "- Head to `File - Download - Download.ipynb` in the top-left corner to submit the notebook on the portal under the `Hand-In Assignment tab`\n",
        "\n",
        "\n",
        "\\\n",
        "\n",
        "Good luck, and we look forward to receiving your answers!\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uaVo2-OFKf0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1\n",
        "\n",
        "The array-form of an integer `num` is an array representing its digits in left to right order.\n",
        "\n",
        "For example, for num = 1321, the array form is [1,3,2,1]\n",
        "\n",
        "Given `num`, the array-form of an integer, and an integer `k`, return the array-form of the integer num + `k`\n",
        "\n",
        "**Extra points!** - throw atleast one list comprehension in your code.\n",
        "\n",
        "----\n",
        "\n",
        "\\\n",
        "\n",
        "input: \n",
        "\n",
        "num = `[1,2,0,0]`\n",
        "\n",
        "k = `34`\n",
        "\n",
        "Output:\n",
        "\n",
        "[1,2,3,4]\n",
        "\n",
        "\n",
        "Constrainsts:\n",
        "\n",
        "- 1 <= num.length <= 10\n",
        "- 0 <=n num[i] <= 9\n",
        "- `num` does not cibtatin any leading zeroes except for the zero itself.\n",
        "- 1 <= k <= 10\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q9jlS_TqY41y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def num_array_to_integer(num):\n",
        "  l = len(num)\n",
        "  for i in num:\n",
        "    if i > 10:\n",
        "      print(\"Invalid Array\")\n",
        "      return\n",
        "  dummy_string = \"\"\n",
        "  if len(num) == 1:\n",
        "    dummy_string = dummy_string + str(num[0])\n",
        "    int_num = int(dummy_string)\n",
        "    return int_num\n",
        "  while l > 1 and l < 11:\n",
        "    if num[0] == 0:\n",
        "      break\n",
        "    else:\n",
        "      for x in num: \n",
        "        if x >= 0 and x <10:\n",
        "          dummy_string = dummy_string + str(x)\n",
        "        int_num = int(dummy_string)\n",
        "    return int_num \n",
        "\n",
        "def num_array_function(num, k):\n",
        "  if k > 0 and k < 11:\n",
        "    int_num = num_array_to_integer(num)\n",
        "    dummy_sum = int_num + k\n",
        "    #print(dummy_sum, type(dummy_sum))\n",
        "    dummy_string = str(dummy_sum)\n",
        "    #print(type(dummy_string), dummy_string)\n",
        "    new_array = [int(x) for x in dummy_string]\n",
        "    return new_array\n",
        "  else:\n",
        "    print(\"k needs to be such that: 1< = k <=10\")\n",
        "\n",
        "\n",
        "num = [1,2,3,9]\n",
        "k = 3\n",
        "print(num_array_function(num,k))\n",
        "\n",
        "#####Testing Area \n",
        "\n",
        "#num = [2]\n",
        "\n",
        "#print(type(num_array_to_integer(num)),num_array_to_integer(num))\n",
        "#print(type(num_array_function(num, k)), num_array_function(num, k))\n",
        "#print(type(num_array_function(num, k)[0]))\\\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "4dhPwk7nY3qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2\n",
        "\n",
        "Given an integer array `nums`, return all the triplets `[nums[i], nums[j], nums[k]]` such that **i != j, i!=k and j!=k**, and **nums[i] + nums[j] + nums[k] == 0**\n",
        "\n",
        "\n",
        "-------\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "Input: nums = `[-1,0,1,2,-1,-4]`\n",
        "\n",
        "Ouput: `[[-1,-1,2],[-1,0,1]]`\n",
        "\n",
        "Constraints:\n",
        "\n",
        "- **0 <= nums.length <= 3000**\n",
        "\n",
        "- **-10^5 <=nums[i] <= 10^5**"
      ],
      "metadata": {
        "id": "SUNk_aEnQYJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " nums = [-1,0,1,2,-1,-4]\n",
        "\n",
        "\n",
        "def validator(nums):\n",
        "  l = len(nums)\n",
        "  if l < 0 or l> 3000:\n",
        "    print(\"the array is not of acceptable length\")\n",
        "    return \n",
        "  for x in nums:\n",
        "    if x < -10**5 or x > 10**5:\n",
        "        print(\"integers out of range\")\n",
        "        return\n",
        "  return nums\n",
        "\n",
        "def triplet_generator(nums):\n",
        "  nums = validator(nums)\n",
        "  n = len(nums)\n",
        "  triplets =[]\n",
        "  for i in range(0, n-2):\n",
        "    for j in range(i+1,n-1):\n",
        "      for k in range(j+1,n):\n",
        "        if nums[i] != nums[j] and nums[j] != nums[k] and nums[i] != nums[k]:\n",
        "          if (nums[i]+ nums[j] + nums[k] == 0):\n",
        "            triplets.append([nums[i],nums[j],nums[k]])\n",
        "    #final_triplets = {x for x in triplets}\n",
        "  return triplets\n",
        "\n",
        "triplet_generator(nums)\n",
        "\n",
        "#note the given example output above is wrong since the triplet [-1,-1,2] violates the given constraint of being mutually unequal\n",
        "#this is probably slow,probably would be better to split it and look for pairs that sum to the negative of the third\n",
        "#I presumed that you would  specify if the triplets were to be unique"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is63FqUzSK8V",
        "outputId": "11df001e-39cd-437f-85d7-fb7ca84c9f0b"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-1, 0, 1], [0, 1, -1]]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3\n",
        "\n",
        "Given the dataframe below, using Numpy alone convert the sex column to a categorical variable and employ the results to return the indices of each female passenger.\n",
        "\n",
        "-----------\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "Expected output: \n",
        "\n",
        "`[1,2,3,4,8...]`\n",
        "\n",
        "\\\n",
        "\n",
        "**bonus round!**\n",
        "\n",
        "Combine your Numpy only solution to select the appropriate rows in the sex column. You may use either Numpy or Pandas to solve this part as long as your indices in part 1 are derived using Numpy.\n",
        "\n",
        "\\\n",
        "\n",
        "1  female\n",
        "\n",
        "2  female\n",
        " \n",
        "3  female\n",
        " \n",
        "8  female\n",
        " \n",
        "9  female\n",
        "    ...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "--V57UwzMZe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "\n",
        "data_url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "df = pd.read_csv(data_url)\n",
        "df.head(20)"
      ],
      "metadata": {
        "id": "kds7sKIaBWmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "myarray = np.array(df)\n",
        "myarray\n",
        "\n",
        "#function to return a list containing the indices of females\n",
        "def indice_returner(myarray):\n",
        "  i = -1 \n",
        "  index_list = []\n",
        "  for x in myarray:\n",
        "    i = i+ 1\n",
        "    if x[3] == \"female\":\n",
        "      index_list.append(i)\n",
        "  return index_list\n",
        "\n",
        "print(indice_returner(myarray))\n",
        "\n",
        "#a function to render indice and gender tuple\n",
        "def bonus_returner(myarray):\n",
        "  i = -1\n",
        "  index_list = []\n",
        "  for x in myarray:\n",
        "    i = i+ 1\n",
        "    if x[3] == \"female\":\n",
        "      index_list.append([i, \"female\"])\n",
        "  new_array = np.array(index_list)\n",
        "  return new_array\n",
        "\n",
        "print(bonus_returner(myarray))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_C7Po_oMk8Q",
        "outputId": "027030df-216d-41a6-94ad-c68f86a1f6f4"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 8, 9, 10, 11, 14, 15, 18, 19, 22, 24, 25, 28, 31, 32, 38, 39, 40, 41, 42, 43, 46, 48, 51, 52, 55, 57, 60, 65, 67, 70, 78, 81, 83, 84, 87, 97, 99, 105, 108, 110, 112, 113, 118, 122, 127, 131, 132, 135, 139, 140, 141, 146, 150, 155, 160, 165, 166, 171, 176, 179, 183, 185, 189, 191, 193, 194, 197, 198, 204, 207, 210, 214, 215, 217, 228, 229, 232, 234, 236, 239, 240, 245, 246, 250, 253, 254, 255, 256, 257, 258, 262, 266, 267, 270, 272, 273, 274, 277, 287, 288, 289, 291, 295, 297, 298, 301, 304, 305, 307, 308, 309, 310, 313, 314, 316, 317, 320, 321, 323, 325, 326, 327, 328, 332, 335, 339, 343, 344, 345, 354, 355, 356, 357, 360, 364, 365, 366, 367, 372, 373, 374, 378, 379, 381, 385, 387, 391, 392, 394, 397, 400, 402, 407, 410, 413, 414, 415, 417, 420, 423, 424, 428, 429, 432, 433, 434, 437, 440, 443, 445, 454, 455, 466, 469, 470, 471, 476, 480, 482, 483, 493, 495, 498, 499, 500, 501, 503, 510, 513, 515, 517, 520, 523, 527, 530, 531, 532, 534, 536, 537, 538, 539, 543, 551, 553, 555, 556, 561, 564, 568, 570, 573, 574, 575, 577, 578, 582, 588, 590, 593, 597, 605, 606, 607, 609, 612, 614, 615, 624, 631, 632, 635, 638, 639, 641, 646, 648, 650, 651, 654, 666, 667, 674, 675, 677, 686, 688, 694, 697, 699, 703, 705, 707, 713, 714, 716, 722, 723, 725, 726, 732, 738, 743, 746, 750, 755, 759, 761, 763, 768, 770, 773, 775, 776, 777, 782, 788, 792, 793, 795, 797, 803, 805, 809, 812, 816, 819, 825, 826, 831, 838, 845, 848, 849, 850, 851, 852, 854, 858, 859, 861, 862, 867, 870, 871, 875, 876, 878, 881, 883, 884]\n",
            "[['1' 'female']\n",
            " ['2' 'female']\n",
            " ['3' 'female']\n",
            " ['8' 'female']\n",
            " ['9' 'female']\n",
            " ['10' 'female']\n",
            " ['11' 'female']\n",
            " ['14' 'female']\n",
            " ['15' 'female']\n",
            " ['18' 'female']\n",
            " ['19' 'female']\n",
            " ['22' 'female']\n",
            " ['24' 'female']\n",
            " ['25' 'female']\n",
            " ['28' 'female']\n",
            " ['31' 'female']\n",
            " ['32' 'female']\n",
            " ['38' 'female']\n",
            " ['39' 'female']\n",
            " ['40' 'female']\n",
            " ['41' 'female']\n",
            " ['42' 'female']\n",
            " ['43' 'female']\n",
            " ['46' 'female']\n",
            " ['48' 'female']\n",
            " ['51' 'female']\n",
            " ['52' 'female']\n",
            " ['55' 'female']\n",
            " ['57' 'female']\n",
            " ['60' 'female']\n",
            " ['65' 'female']\n",
            " ['67' 'female']\n",
            " ['70' 'female']\n",
            " ['78' 'female']\n",
            " ['81' 'female']\n",
            " ['83' 'female']\n",
            " ['84' 'female']\n",
            " ['87' 'female']\n",
            " ['97' 'female']\n",
            " ['99' 'female']\n",
            " ['105' 'female']\n",
            " ['108' 'female']\n",
            " ['110' 'female']\n",
            " ['112' 'female']\n",
            " ['113' 'female']\n",
            " ['118' 'female']\n",
            " ['122' 'female']\n",
            " ['127' 'female']\n",
            " ['131' 'female']\n",
            " ['132' 'female']\n",
            " ['135' 'female']\n",
            " ['139' 'female']\n",
            " ['140' 'female']\n",
            " ['141' 'female']\n",
            " ['146' 'female']\n",
            " ['150' 'female']\n",
            " ['155' 'female']\n",
            " ['160' 'female']\n",
            " ['165' 'female']\n",
            " ['166' 'female']\n",
            " ['171' 'female']\n",
            " ['176' 'female']\n",
            " ['179' 'female']\n",
            " ['183' 'female']\n",
            " ['185' 'female']\n",
            " ['189' 'female']\n",
            " ['191' 'female']\n",
            " ['193' 'female']\n",
            " ['194' 'female']\n",
            " ['197' 'female']\n",
            " ['198' 'female']\n",
            " ['204' 'female']\n",
            " ['207' 'female']\n",
            " ['210' 'female']\n",
            " ['214' 'female']\n",
            " ['215' 'female']\n",
            " ['217' 'female']\n",
            " ['228' 'female']\n",
            " ['229' 'female']\n",
            " ['232' 'female']\n",
            " ['234' 'female']\n",
            " ['236' 'female']\n",
            " ['239' 'female']\n",
            " ['240' 'female']\n",
            " ['245' 'female']\n",
            " ['246' 'female']\n",
            " ['250' 'female']\n",
            " ['253' 'female']\n",
            " ['254' 'female']\n",
            " ['255' 'female']\n",
            " ['256' 'female']\n",
            " ['257' 'female']\n",
            " ['258' 'female']\n",
            " ['262' 'female']\n",
            " ['266' 'female']\n",
            " ['267' 'female']\n",
            " ['270' 'female']\n",
            " ['272' 'female']\n",
            " ['273' 'female']\n",
            " ['274' 'female']\n",
            " ['277' 'female']\n",
            " ['287' 'female']\n",
            " ['288' 'female']\n",
            " ['289' 'female']\n",
            " ['291' 'female']\n",
            " ['295' 'female']\n",
            " ['297' 'female']\n",
            " ['298' 'female']\n",
            " ['301' 'female']\n",
            " ['304' 'female']\n",
            " ['305' 'female']\n",
            " ['307' 'female']\n",
            " ['308' 'female']\n",
            " ['309' 'female']\n",
            " ['310' 'female']\n",
            " ['313' 'female']\n",
            " ['314' 'female']\n",
            " ['316' 'female']\n",
            " ['317' 'female']\n",
            " ['320' 'female']\n",
            " ['321' 'female']\n",
            " ['323' 'female']\n",
            " ['325' 'female']\n",
            " ['326' 'female']\n",
            " ['327' 'female']\n",
            " ['328' 'female']\n",
            " ['332' 'female']\n",
            " ['335' 'female']\n",
            " ['339' 'female']\n",
            " ['343' 'female']\n",
            " ['344' 'female']\n",
            " ['345' 'female']\n",
            " ['354' 'female']\n",
            " ['355' 'female']\n",
            " ['356' 'female']\n",
            " ['357' 'female']\n",
            " ['360' 'female']\n",
            " ['364' 'female']\n",
            " ['365' 'female']\n",
            " ['366' 'female']\n",
            " ['367' 'female']\n",
            " ['372' 'female']\n",
            " ['373' 'female']\n",
            " ['374' 'female']\n",
            " ['378' 'female']\n",
            " ['379' 'female']\n",
            " ['381' 'female']\n",
            " ['385' 'female']\n",
            " ['387' 'female']\n",
            " ['391' 'female']\n",
            " ['392' 'female']\n",
            " ['394' 'female']\n",
            " ['397' 'female']\n",
            " ['400' 'female']\n",
            " ['402' 'female']\n",
            " ['407' 'female']\n",
            " ['410' 'female']\n",
            " ['413' 'female']\n",
            " ['414' 'female']\n",
            " ['415' 'female']\n",
            " ['417' 'female']\n",
            " ['420' 'female']\n",
            " ['423' 'female']\n",
            " ['424' 'female']\n",
            " ['428' 'female']\n",
            " ['429' 'female']\n",
            " ['432' 'female']\n",
            " ['433' 'female']\n",
            " ['434' 'female']\n",
            " ['437' 'female']\n",
            " ['440' 'female']\n",
            " ['443' 'female']\n",
            " ['445' 'female']\n",
            " ['454' 'female']\n",
            " ['455' 'female']\n",
            " ['466' 'female']\n",
            " ['469' 'female']\n",
            " ['470' 'female']\n",
            " ['471' 'female']\n",
            " ['476' 'female']\n",
            " ['480' 'female']\n",
            " ['482' 'female']\n",
            " ['483' 'female']\n",
            " ['493' 'female']\n",
            " ['495' 'female']\n",
            " ['498' 'female']\n",
            " ['499' 'female']\n",
            " ['500' 'female']\n",
            " ['501' 'female']\n",
            " ['503' 'female']\n",
            " ['510' 'female']\n",
            " ['513' 'female']\n",
            " ['515' 'female']\n",
            " ['517' 'female']\n",
            " ['520' 'female']\n",
            " ['523' 'female']\n",
            " ['527' 'female']\n",
            " ['530' 'female']\n",
            " ['531' 'female']\n",
            " ['532' 'female']\n",
            " ['534' 'female']\n",
            " ['536' 'female']\n",
            " ['537' 'female']\n",
            " ['538' 'female']\n",
            " ['539' 'female']\n",
            " ['543' 'female']\n",
            " ['551' 'female']\n",
            " ['553' 'female']\n",
            " ['555' 'female']\n",
            " ['556' 'female']\n",
            " ['561' 'female']\n",
            " ['564' 'female']\n",
            " ['568' 'female']\n",
            " ['570' 'female']\n",
            " ['573' 'female']\n",
            " ['574' 'female']\n",
            " ['575' 'female']\n",
            " ['577' 'female']\n",
            " ['578' 'female']\n",
            " ['582' 'female']\n",
            " ['588' 'female']\n",
            " ['590' 'female']\n",
            " ['593' 'female']\n",
            " ['597' 'female']\n",
            " ['605' 'female']\n",
            " ['606' 'female']\n",
            " ['607' 'female']\n",
            " ['609' 'female']\n",
            " ['612' 'female']\n",
            " ['614' 'female']\n",
            " ['615' 'female']\n",
            " ['624' 'female']\n",
            " ['631' 'female']\n",
            " ['632' 'female']\n",
            " ['635' 'female']\n",
            " ['638' 'female']\n",
            " ['639' 'female']\n",
            " ['641' 'female']\n",
            " ['646' 'female']\n",
            " ['648' 'female']\n",
            " ['650' 'female']\n",
            " ['651' 'female']\n",
            " ['654' 'female']\n",
            " ['666' 'female']\n",
            " ['667' 'female']\n",
            " ['674' 'female']\n",
            " ['675' 'female']\n",
            " ['677' 'female']\n",
            " ['686' 'female']\n",
            " ['688' 'female']\n",
            " ['694' 'female']\n",
            " ['697' 'female']\n",
            " ['699' 'female']\n",
            " ['703' 'female']\n",
            " ['705' 'female']\n",
            " ['707' 'female']\n",
            " ['713' 'female']\n",
            " ['714' 'female']\n",
            " ['716' 'female']\n",
            " ['722' 'female']\n",
            " ['723' 'female']\n",
            " ['725' 'female']\n",
            " ['726' 'female']\n",
            " ['732' 'female']\n",
            " ['738' 'female']\n",
            " ['743' 'female']\n",
            " ['746' 'female']\n",
            " ['750' 'female']\n",
            " ['755' 'female']\n",
            " ['759' 'female']\n",
            " ['761' 'female']\n",
            " ['763' 'female']\n",
            " ['768' 'female']\n",
            " ['770' 'female']\n",
            " ['773' 'female']\n",
            " ['775' 'female']\n",
            " ['776' 'female']\n",
            " ['777' 'female']\n",
            " ['782' 'female']\n",
            " ['788' 'female']\n",
            " ['792' 'female']\n",
            " ['793' 'female']\n",
            " ['795' 'female']\n",
            " ['797' 'female']\n",
            " ['803' 'female']\n",
            " ['805' 'female']\n",
            " ['809' 'female']\n",
            " ['812' 'female']\n",
            " ['816' 'female']\n",
            " ['819' 'female']\n",
            " ['825' 'female']\n",
            " ['826' 'female']\n",
            " ['831' 'female']\n",
            " ['838' 'female']\n",
            " ['845' 'female']\n",
            " ['848' 'female']\n",
            " ['849' 'female']\n",
            " ['850' 'female']\n",
            " ['851' 'female']\n",
            " ['852' 'female']\n",
            " ['854' 'female']\n",
            " ['858' 'female']\n",
            " ['859' 'female']\n",
            " ['861' 'female']\n",
            " ['862' 'female']\n",
            " ['867' 'female']\n",
            " ['870' 'female']\n",
            " ['871' 'female']\n",
            " ['875' 'female']\n",
            " ['876' 'female']\n",
            " ['878' 'female']\n",
            " ['881' 'female']\n",
            " ['883' 'female']\n",
            " ['884' 'female']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4\n",
        "\n",
        "Given the following sentence:\n",
        "\n",
        "**'Hello! We are super excited to have you join our AI bootcamp to learn about NLP amongst other things!'**\n",
        "\n",
        "package the word count sans punctuation into a `{'word1': count,...}` dictionary\n",
        "\n",
        "\n",
        "-------\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "\n",
        "input: \n",
        "\n",
        "str\n",
        "\n",
        "Constraints:\n",
        "\n",
        "Ensure stop words and punctuation are stripped. Example incorrect output below :-\n",
        "\n",
        "`{'hello!': 1, \"we're\": 1, 'super': 1, 'excited': 1, 'to': 1, 'have': 1, 'you': 1, 'join': 1, 'our': 1, 'ai': 1, 'bootcamp': 1, 'learn': 1, 'about': 1, 'nlp': 1, 'amongst': 1, 'other': 1, 'things!': 1`\n",
        "\n",
        "output: \n",
        "\n",
        "`{'hello': 1, 'super': 1, 'excited': 1, 'join': 1, 'bootcamp': 1, 'learn': 1, 'nlp': 1, 'amongst': 1, 'things': 1}`\n",
        "\n",
        "\n",
        "**bonus round** - Once again extra props if you can squeeze a handy list comprehension in there for speed and efficiency."
      ],
      "metadata": {
        "id": "jy9EAjeNndk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greeting = 'Hello! We are re super excited to have you join our AI bootcamp to learn about NLP amongst other things!'\n",
        "\n",
        "#your answer here\n",
        "import re\n",
        "bad_words = [\"we\",\"are\",\"to\",\"you\",\"have\", \"our\",\"about\",\"other\", \"re\", \"we\", \"ai\"]\n",
        "\n",
        "def word_packager(my_string):\n",
        "  dummy_string = my_string.lower().strip()\n",
        "  print(dummy_string)\n",
        "  word_dict = {}\n",
        "  pattern = r\"\\w+\"\n",
        "  middleman = re.findall(pattern,dummy_string)\n",
        "  for x in middleman:\n",
        "    if x not in bad_words:\n",
        "      if x not in word_dict.keys():\n",
        "        word_dict[x] = 1\n",
        "      else:\n",
        "        word_dict[x] =+ x\n",
        "  return word_dict\n",
        "\n",
        "word_packager(greeting)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nkr-XLFmodsT",
        "outputId": "ead1029d-e1a1-4526-bc57-21cb4fffa57a"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello! we are re super excited to have you join our ai bootcamp to learn about nlp amongst other things!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'amongst': 1,\n",
              " 'bootcamp': 1,\n",
              " 'excited': 1,\n",
              " 'hello': 1,\n",
              " 'join': 1,\n",
              " 'learn': 1,\n",
              " 'nlp': 1,\n",
              " 'super': 1,\n",
              " 'things': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    }
  ]
}